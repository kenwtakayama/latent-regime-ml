# Comparing Machine Learning Methods for Latent Regime Detection in Multivariate Time Series

## 1. Motivation

Many real-world systems generate multivariate time series that exhibit **nonstationarity** and **regime-switching behavior**, where the underlying data-generating process changes over time. Examples include physical systems, biological signals, sensor networks, and economic or financial data.

A central challenge is identifying these latent regimes **without strong domain-specific assumptions**. This project investigates how different machine learning models perform at detecting latent regimes in multivariate time series, with an emphasis on **interpretability**, **robustness**, and **methodological comparison** rather than domain-specific forecasting performance.

The primary goal is to understand *which modeling choices matter*, *what kinds of structure each model can recover*, and *how reliably regimes can be identified under noise and correlation*.

---

## 2. Research Questions

* Can unsupervised and weakly supervised machine learning models recover latent regimes in multivariate time series?
* How do classical statistical methods compare to modern ML and shallow deep learning approaches?
* Which features and temporal patterns most strongly influence regime identification?

---

## 3. Data

### 3.1 Synthetic Data (Primary Validation)

To avoid domain-specific assumptions and provide ground truth, we generate synthetic multivariate time series with:

* Piecewise constant regimes
* Regime-dependent mean and covariance
* Correlated noise across features
* Stochastic regime transitions

This allows direct evaluation of regime recovery accuracy and robustness under controlled conditions.

### 3.2 Real-World Time Series (Secondary Application)

After validating models on synthetic data, we apply the same pipeline to a real multivariate time series dataset treated as **anonymous signals**. No domain-specific interpretation is assumed; the data serves only to test generalization behavior.

All features are standardized, and sliding-window representations are used where appropriate.

---

## 4. Methods

### 4.1 Baseline Approaches

* Rolling window statistics (mean, variance)
* Principal Component Analysis (PCA)
* KMeans clustering on reduced representations

### 4.2 Probabilistic Models

* Gaussian Mixture Models (GMM)
* Hidden Markov Models (HMM) with Gaussian emissions

These models explicitly encode latent state structure and transition dynamics.

### 4.3 Supervised and ML Models

* Logistic regression on windowed features
* Random Forest classifier

These serve as flexible non-linear baselines without explicit temporal state modeling.

### 4.4 Shallow Deep Learning

* Multi-layer Perceptron (MLP) implemented in PyTorch

The neural model is intentionally kept shallow to focus on representation learning rather than scale.

---

## 5. Evaluation

### Synthetic Data

* Regime recovery accuracy
* Adjusted Rand Index (ARI)
* Transition matrix estimation error

### Real Data

* Regime stability and persistence
* Consistency across model classes
* Sensitivity to window size and feature subsets

Ablation studies are used to assess robustness to noise, feature removal, and regime duration.

---

## 6. Interpretability

Interpretability is a core focus of this project. We analyze:

* Feature importance (tree-based models)
* Permutation importance
* Regime-wise feature statistics
* Estimated transition matrices and regime durations

These analyses help connect learned regimes to observable data characteristics.

---

## 7. Results Summary

Across experiments, probabilistic models with explicit state structure perform best at recovering true regimes in synthetic data, while tree-based and neural models provide competitive performance with greater flexibility. Interpretability analyses reveal that regime identification is often driven by variance structure and cross-feature correlations rather than marginal means alone.

---

## 8. Limitations

* Synthetic data may not capture all complexities of real-world systems
* Shallow neural architectures limit representational capacity
* No causal interpretation of regimes is attempted

---

## 9. Future Work

* Bayesian state-space models
* Variational inference for regime uncertainty
* Multiscale temporal modeling
* Extension to higher-dimensional and irregularly sampled data

---

## 10. Reproducibility

All experiments are fully reproducible. Random seeds are fixed, and all data generation and training scripts are provided. Figures can be regenerated by running the notebooks in order.

---

## 11. Repository Structure

```
latent-regime-ml/
├── data/
├── notebooks/
│   ├── 01_synthetic_generation.ipynb
│   ├── 02_exploration.ipynb
│   ├── 03_baselines.ipynb
│   ├── 04_ml_models.ipynb
│   └── 05_interpretability.ipynb
├── src/
│   ├── data.py
│   ├── models.py
│   ├── train.py
│   └── utils.py
├── figures/
└── README.md
```

---

## 12. Key Takeaway

This project emphasizes **methodological clarity**, **interpretability**, and **controlled validation** over domain-specific assumptions, demonstrating how machine learning models can uncover latent structure in complex time series data.

